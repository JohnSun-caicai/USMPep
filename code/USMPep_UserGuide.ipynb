{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib  inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from modelv1 import Model    \n",
    "from proteomics_preprocessing import *\n",
    "from utils.proteomics_utils import *\n",
    "from mhc_analysis_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Requirements\n",
    "\n",
    "### Swiss-Prot\n",
    "Download and extract the desired Swiss-Prot release (we use 2018_10) from the UniProt ftp server. Save the contained uniprot_sprot.fasta in the ./data directory\n",
    "\n",
    "### MHCFlurry18 [ODonnell2018]\n",
    "Download curated MHC I dataset by [ODonnell2018] from https://data.mendeley.com/datasets/8pz43nvvxh/1/files/e1916ecf-b544-40e6-b1fe-e0024bea76a7/data_curated.20180219.tar.bz2?dl=1 and extract the archive in ./data\n",
    "\n",
    "### IEDB16_I [Zhao2018]\n",
    "Download the curated MHC I test data set by [Zhao2019] from https://doi.org/10.1371/journal.pcbi.1006457.s009 and extract the archive as well as the contained archives in the ./data directory\n",
    "\n",
    "### HPV [Bonsack2019]\n",
    "For the user's convenience the data from table S2 from Supplementary Material of [Bonsack2019] is provided as HPV_data.csv in ./git_data\n",
    "\n",
    "### Kim14 [Kim2014]\n",
    "Download the curated MHC I dataset by [Kim2014] from http://tools.iedb.org/static/main/benchmark_mhci_reliability.tar.gz and extract the archive in ./data\n",
    "\n",
    "### Wang10 [Wang10]\n",
    "Download the MHC II dataset prepared by [Wang2010] from http://tools.iedb.org/static/download/classII_binding_data_Nov_16_2009.tar.gz and extract the archive in ./data\n",
    "\n",
    "###  IEDB16_II [Zhao2018]\n",
    "Download the curated MHC I test data set by [Zhao2019] from https://doi.org/10.1371/journal.pcbi.1006457.s010 and extract the archive in the ./data directory\n",
    "\n",
    "\n",
    "**References**\n",
    "- [ODonnell2018] T. J. O’Donnell, A. Rubinsteyn, M. Bonsack, A. B. Riemer, U. Laserson, and J. Hammerbacher, “MHCflurry: Open-Source Class I MHC Binding Affinity Prediction,” Cell Systems, vol. 7, no. 1, pp. 129–132.e4, Jul. 2018. [Online].\n",
    "Available: https://doi.org/10.1016/j.cels.2018.05.014\n",
    "- [Zhao2018] W. Zhao and X. Sher, “Systematically benchmarking peptide-MHC binding predictors: From synthetic to naturally processed epitopes,” PLOS Computational Biology, vol. 14, no. 11, p. e1006457, Nov. 2018. [Online]. Available:https://doi.org/10.1371/journal.pcbi.1006457\n",
    "- [Kim2014] Y. Kim, J. Sidney, S. Buus, A. Sette, M. Nielsen, and B. Peters, “Dataset size and composition impact the reliability of performance benchmarks for peptide-MHC binding predictions,” BMC Bioinformatics, vol. 15, no. 1, p. 241, 2014. [Online]. Available: https://doi.org/10.1186/1471-2105-15-241\n",
    "- [Bonsack2019] M. Bonsack, S. Hoppe, J. Winter, D. Tichy, C. Zeller, M. D. Küpper, E. C. Schitter, R. Blatnik, and A. B. Riemer, “Performance Evaluation of MHC Class-I Binding Prediction Tools Based on an Experimentally Validated MHC–Peptide Binding Data Set,” Cancer Immunology Research, vol. 7, no. 5, pp. 719–736, Mar. 2019. [Online]. Available: https://doi.org/10.1158/2326-6066.cir-18-0584\n",
    "- [Wang2010] P. Wang, J. Sidney, Y. Kim, A. Sette, O. Lund, M. Nielsen, and B. Peters, “Peptide binding predictions for HLA DR, DP and DQ molecules,” BMC Bioinformatics, vol. 11, no. 1, p. 568, 2010. [Online]. Available: https://doi.org/10.1186/1471-2105-11-568"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binding affinity datasets\n",
    "Create a directory for each dataset with subdirectories for each allele. The output of the preprocessing saved in each allele subdirectory is structured as follows:\n",
    "\n",
    "- *tok.npy* sequences as list of numerical indices (mapping is provided by *tok_itos.npy*)\n",
    "- *label.npy* label as list of binding affintiy values (mapping is provided by *label_itos.npy*)\n",
    "- *train_IDs.npy/val_IDs.npy/test_IDs.npy* numerical indices identifying training/validation/test set by specifying rows in tok.npy\n",
    "- *train_IDs_prev.npy/val_IDs_prev.npy/test_IDs_prev.npy* original non-numerical IDs for all entries that were ever assigned to the respective sets (used to obtain consistent splits for downstream tasks)\n",
    "- *ID.npy* original non-numerical IDs for all entries in tok.npy\n",
    "\n",
    "To ease the handling of a multitude of different alleles, they are ranked by the number of peptides present in the respective training dataset. Below, the allele subsirectories are therefore named \"allelex\" fot the xth allele in the ranking, except for the HPV dataset where the subdirectories are named after the alleles directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEDB16_I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path= Path(\"./reg_IEDB16_I\")\n",
    "dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "prep = Preprocess()\n",
    "for allele in np.arange(0,34).astype(int):\n",
    "    prep.clas_mhc_i_zhao(allele, working_folder=dataset_path/\"allele{}\".format(allele),\n",
    "                         pretrained_folder=\"../git_data/lm_netchop_peptides\", train_set=\"MHCFlurry18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### HPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path= Path(\"./reg_HPV\")\n",
    "dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "prep = Preprocess()\n",
    "for allele in ['HLAA1','HLAA1', 'HLAA2', 'HLAA3', 'HLAA11', 'HLAA24', 'HLAB7', 'HLAB15']:\n",
    "    prep.clas_mhc_i_hpv(allele, working_folder=dataset_path/allele, pretrained_folder=\"../git_data/lm_netchop_peptides\", \n",
    "                        train_set=\"MHCFlurry18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Kim14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path= Path(\"./reg_Kim14\")\n",
    "dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "prep = Preprocess()\n",
    "for allele in np.arange(0,53).astype(int):\n",
    "    prep.clas_mhc_kim(allele, working_folder=dataset_path/\"allele{}\".format(allele), \n",
    "                  pretrained_folder=\"../git_data/lm_netchop_peptides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEDB16_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path= Path(\"./reg_IEDB16_II\")\n",
    "dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "prep = Preprocess()\n",
    "for allele in range(0,24):\n",
    "    prep.clas_mhc_ii(allele, working_folder=dataset_path/\"allele{}\".format(allele), \n",
    "                     pretrained_folder=\"../git_data/lm_netchop_peptides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NetChop sliced peptides form LM pretraining (optional)\n",
    "To obtain protesome sliced sequences for language model pretraining, the following steps are taken:\n",
    "\n",
    "1. Use NetChop from http://www.cbs.dtu.dk/services/NetChop/ to obtain cleavage sites for proteines from a *.fasta file \n",
    "    -**This can take a long time**\n",
    "2. Slice proteines with the cleavage probability provided by NetChop\n",
    "3. Tokenize the peptides\n",
    "4. Perform train-validation-test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_sprot_fasta = Path('../data/uniprot_sprot.fasta')\n",
    "# filter for human proteome\n",
    "df_sprot = parse_uniprot_fasta(path_sprot_fasta)\n",
    "human_sprot = filter_human_proteome(df_sprot)\n",
    "df_to_fasta(human_sprot, './data/uniprot_sprot_human.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify the path of the NetChop tcsh\n",
    "netchop_path= '../../../software/NetChop/netchop-3.1/netchop'\n",
    "# path for human proteome *.fasta file\n",
    "protein_fasta_file = '../data/uniprot_sprot_human.fasta'\n",
    "\n",
    "\n",
    "# Load and clean proteins, run NetChop as subprocess, slice peptides according to NetChop output, save tokenized peptides\n",
    "load = Preprocess()\n",
    "load.lm_netchop(working_folder=\"./lm_netchop_peptides\", existing_netchop_peptides=None, netchop_path=netchop_path,\n",
    "                protein_fasta_file=protein_fasta_file,\n",
    "                netchop_min_length=8, netchop_max_length=20, netchop_repeats=30,\n",
    "                ignore_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Training and Evaluation\n",
    "\n",
    "from_scratch is set to True for training from scratch and set to False for using a language model. By default we will use the provided language model that was pretrained on Netchop peptides (../git_data/lm_netchop_peptides)\n",
    "\n",
    "The output is logged in logfile.log in the working directory. If eval_on_test and export_preds are set to True, the individual predictions on the test set are exported as preds_valid.npz.\n",
    "If eval_on_val_test and export_preds are set to true, the validation test predictions are stored as preds_valid.npz and the test predictions are stored as preds_test.npz.\n",
    "\n",
    "**CHOOSE A DATASET**\n",
    "\n",
    "Choices: \n",
    "- IEDB16_I\n",
    "- HPV\n",
    "- Kim14\n",
    "- IEDB16_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a dataset:\n",
    "dataset = \"HPV\"\n",
    "\n",
    "assert dataset in [\"IEDB16_I\", \"HPV\", \"Kim14\", \"IEDB16_II\"], 'dataset not in [\"IEDB16_I\", \"HPV\", \"Kim14\", \"IEDB16_II\"]' \n",
    "\n",
    "data_dir = \"./reg_{}\".format(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Model for one allele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = data_dir+\"/allele0\" if dataset!=\"HPV\" else data_dir+\"/HLAA1\"\n",
    "modelv1=Model()\n",
    "\n",
    "modelv1.generic_model(working_folder=working_folder, model_filename_prefix=\"fs_single\",\n",
    "                from_scratch=True,\n",
    "                eval_on_test=True, export_preds=True,\n",
    "                train=True,clas=True, regression=True, concat_train_val=True,\n",
    "                emb_sz=50,nh=64,nl=1,\n",
    "                 bs=32, epochs=10, lr=0.05,\n",
    "                 wd=1e-7, dropout=0.1,\n",
    "                 interactive=False,\n",
    "                metrics=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "working_folder = data_dir+\"/allele0\" if dataset!=\"HPV\" else data_dir+\"/HLAA1\"\n",
    "modelv1=Model()\n",
    "\n",
    "modelv1.generic_model(working_folder=working_folder, model_filename_prefix=\"lm_single\",\n",
    "                from_scratch=False,pretrained_folder=\"../git_data/lm_netchop_peptides\",pretrained_model_filename=\"lm_1l_3_enc\",\n",
    "                eval_on_test=True, export_preds=True,\n",
    "                train=True,clas=True, regression=True, concat_train_val=True,\n",
    "                emb_sz=50,nh=64,nl=1,\n",
    "                 bs=32, epochs=10, lr=0.05,\n",
    "                 wd=1e-7, dropout=0.1,\n",
    "                 interactive=False,\n",
    "                metrics=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "Train an ensemble of predictors for each allele.\n",
    "\n",
    "To evaluate the ensembles, the exported predictions are collected for all alleles and stored as fs_ens_i.npz at after each ensemble member has been trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelv1=Model()\n",
    "\n",
    "allele_dir_list = [os.path.join(data_dir, o) for o in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,o))]\n",
    "n_alleles = len(allele_dir_list)\n",
    "\n",
    "for ensemble_i in range(10):\n",
    "    for clas_folder in allele_dir_list:\n",
    "        # Train\n",
    "        # model_filename_prefix should end on the ensemble index ensemble_i\n",
    "        modelv1.generic_model(working_folder=clas_folder, model_filename_prefix=\"fs_ens_{}\".format(ensemble_i),\n",
    "                        from_scratch=True,\n",
    "                        eval_on_test=True, export_preds=True,\n",
    "                        train=True,clas=True, regression=True, concat_train_val=True,\n",
    "                        emb_sz=50,nh=64,nl=1,\n",
    "                        bs=32, epochs=10, lr=0.05,\n",
    "                        wd=1e-7, dropout=0.1,\n",
    "                        interactive=False,\n",
    "                       metrics=[])\n",
    "        \n",
    "    # be careful to give the correct preds_filename. If eval_on_test=True, the test predictions are saved as \"preds_valid.npz\",\n",
    "    # if eval_on_val_test=True, the test predictions are saved as \"preds_test.npz\",\n",
    "    # val_on_test=True, the test predictions are saved as \"preds_valid.npz\"\n",
    "    if dataset==\"HPV\":\n",
    "        collect_preds_npz(data_dir, n_alleles, subfoldername=\"\", ensemble_i=ensemble_i, preds_filename='preds_valid.npz', \n",
    "                          ranked_alleles=False, allele_list=['HLAA1', 'HLAA24', 'HLAB7', 'HLAA3', 'HLAA11', 'HLAA2', 'HLAB15'])        \n",
    "    else:\n",
    "        collect_preds_npz(data_dir, n_alleles, subfoldername=\"allele\", ensemble_i=ensemble_i, preds_filename='preds_valid.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelv1=Model()\n",
    "\n",
    "allele_dir_list = [os.path.join(data_dir, o) for o in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,o))]\n",
    "n_alleles = len(allele_dir_list)\n",
    "\n",
    "for ensemble_i in range(10):\n",
    "    for clas_folder in allele_dir_list:\n",
    "        # Train\n",
    "        # model_filename_prefix should end on the ensemble index ensemble_i\n",
    "        modelv1.generic_model(working_folder=clas_folder, model_filename_prefix=\"lm_ens_{}\".format(ensemble_i),\n",
    "                        from_scratch=False, \n",
    "                        pretrained_folder=\"../git_data/lm_netchop_peptides\",pretrained_model_filename=\"lm_1l_3_enc\",\n",
    "                        eval_on_test=True, export_preds=True,\n",
    "                        train=True,clas=True, regression=True, concat_train_val=True,\n",
    "                        emb_sz=50,nh=64,nl=1,\n",
    "                        bs=32, epochs=10, lr=0.05,\n",
    "                        wd=1e-7, dropout=0.1,\n",
    "                        interactive=False,\n",
    "                       metrics=[])\n",
    "    # be careful to give the correct preds_filename. If eval_on_test=True, the test predictions are saved as \"preds_valid.npz\",\n",
    "    #  if eval_on_val_test=True, the test predictions are saved as \"preds_test.npz\",\n",
    "    if dataset==\"HPV\":\n",
    "        collect_preds_npz(data_dir, n_alleles, subfoldername=\"\", ensemble_i=ensemble_i, preds_filename='preds_valid.npz', \n",
    "                          ranked_alleles=False, allele_list=['HLAA1', 'HLAA24', 'HLAB7', 'HLAA3', 'HLAA11', 'HLAA2', 'HLAB15'])        \n",
    "    else:\n",
    "        collect_preds_npz(data_dir, n_alleles, subfoldername=\"allele\", ensemble_i=ensemble_i, preds_filename='preds_valid.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "To evaluate the predictions after an ensemble of models has been trained, load the *.npz files and compile model names, predictions, targets, sequences and allele names in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions for all alleles per ensemble\n",
    "ensemble = []\n",
    "for f in Path(data_dir).glob(\"*.npz\"):\n",
    "    print(f)\n",
    "    df_tmp = read_npz(f, ensemble=True)\n",
    "    ensemble.append(df_tmp)\n",
    "    \n",
    "# aggregate ensembles\n",
    "ensemble = pd.concat(ensemble, ignore_index=True, sort=True)\n",
    "\n",
    "columns = ['rank', 'ID', 'targs', 'inequality', 'model'] if dataset!=\"HPV\" else ['rank', 'ID', 'targs', 'model']\n",
    "agg_dic = {}\n",
    "\n",
    "def first_arg(x):\n",
    "    return x.iloc[0]\n",
    "\n",
    "for c in columns:\n",
    "    agg_dic[c] = first_arg\n",
    "    \n",
    "agg_dic['preds'] = np.mean\n",
    "\n",
    "# keep single model predictions\n",
    "single = ensemble.copy()[columns + ['preds']]\n",
    "single = single.set_index([\"model\", \"rank\", \"ID\"])\n",
    "\n",
    "# simply average predicitons for an ensemble predictor\n",
    "ensemble[\"model\"] = ensemble[\"model\"].apply(lambda x: x[:-2])\n",
    "ensemble= ensemble.groupby([\"model\", \"rank\", \"ID\"]).agg(agg_dic)\n",
    "ensemble = ensemble.set_index([\"model\", \"rank\", \"ID\"])\n",
    "\n",
    "result = pd.concat([ensemble, single], sort=True)\n",
    "\n",
    "# Load tokenized sequences to merge with predictions\n",
    "allele_dir_list = [os.path.join(data_dir, o) for o in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,o))]\n",
    "df_seq = []\n",
    "for allele_dir in allele_dir_list:\n",
    "    allele = int(allele_dir.split(\"/\")[-1][6:]) if dataset!=\"HPV\" else allele_dir.split(\"/\")[-1]\n",
    "    df_seq.append(load_tokenized_sequences(allele_dir,allele))\n",
    "df_seq = pd.concat(df_seq, ignore_index=True)\n",
    "\n",
    "result = result.reset_index()\n",
    "if \"rank\" in result.columns and dataset!=\"HPV\":\n",
    "    result = result.astype({\"rank\": int})\n",
    "result= result.merge(df_seq,how=\"inner\", on=[\"rank\",\"ID\"])\n",
    "\n",
    "# Get ranking and merge to restore allele names\n",
    "if dataset!=\"HPV\":\n",
    "    ranking = load_ranking(dataset)\n",
    "    print(result.shape)\n",
    "    result = result.merge(ranking, how=\"inner\", on=\"rank\")\n",
    "    print(result.shape)\n",
    "result = result.set_index([\"model\", \"rank\", \"ID\"]).sort_index()\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "if dataset==\"IEDB16_I\":\n",
    "    result[\"mer\"] = result[\"sequence\"].apply(len)\n",
    "    result_9mer = result [result[\"mer\"]==9]\n",
    "\n",
    "    #overall:\n",
    "    print(\"overall AUC ROC\",result_9mer.groupby(level=0).apply(aucroc_ic50))\n",
    "    print(\"\\noverall Spearman r\",result_9mer.groupby(level=0).apply(spearmanr_eval))\n",
    "    #mean\n",
    "    print(\"\\nmean AUC ROC\",result_9mer.groupby(level=[0,1]).apply(aucroc_ic50).groupby(level=0).mean())\n",
    "    print(\"\\nmean Spearman r\",result_9mer.groupby(level=[0,1]).apply(spearmanr_eval).groupby(level=0).mean())\n",
    "elif dataset==\"Kim14\" or dataset==\"IEDB16_II\":\n",
    "    #overall:\n",
    "    print(\"overall AUC ROC\",result.groupby(level=0).apply(aucroc_ic50))\n",
    "    print(\"\\noverall Spearman r\",result.groupby(level=0).apply(spearmanr_eval))\n",
    "    #mean\n",
    "    print(\"\\nmean AUC ROC\",result.groupby(level=[0,1]).apply(aucroc_ic50).groupby(level=0).mean())\n",
    "    print(\"\\nmean Spearman r\",result.groupby(level=[0,1]).apply(spearmanr_eval).groupby(level=0).mean())\n",
    "elif dataset==\"HPV\":\n",
    "    #overall:\n",
    "    print(\"overall AUC ROC\", result.groupby(level=0).apply(aucroc_hpv))\n",
    "    #mean:\n",
    "    print(\"\\nmean AUC ROC\", result.groupby(level=[0,1]).apply(aucroc_hpv).groupby(level=0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Training (optional- uses pretrained model by default)\n",
    "\n",
    "Train a language model on the netchop sliced proteins generated above.\n",
    "\n",
    "When interactive is set to True, the learning rate can be determined by a plot of the loss against the learning rate after training for one epoch (https://docs.fast.ai/callbacks.lr_finder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_folder = \"./lm_netchop_peptides\"\n",
    "modelv1=Model()\n",
    "\n",
    "modelv1.generic_model(clas=False, \n",
    "                     working_folder=lm_folder, \n",
    "                     model_filename_prefix=\"lm\",\n",
    "                     emb_sz=50,nh=128,nl=1,\n",
    "                     bs=128, epochs=20, lr=0.007,\n",
    "                     dropout=0.5,\n",
    "                     early_stopping=\"accuracy\",\n",
    "                     interactive=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:proteomics]",
   "language": "python",
   "name": "conda-env-proteomics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
